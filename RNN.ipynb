{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd035308-996c-44cc-bda3-37681585fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version.cuda: 12.8\n",
      "torch.cuda.is_available(): False\n",
      "‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import calendar\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU (–∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ —Ä–∞–Ω—å—à–µ)\n",
    "# –ï—Å–ª–∏ CUDA –µ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë. –ï—Å–ª–∏ –Ω–µ—Ç - –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"torch.version.cuda: {torch.version.cuda}\")       # –î–æ–ª–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å '12.4' –∏–ª–∏ '12.1' (–Ω–µ None!)\n",
    "print(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\") # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å True\n",
    "print(f\"torch.xpu.is_available(): {torch.xpu.is_available()}\") \n",
    "\n",
    "print(f\"‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bfafb8-4f0a-4b8b-b468-dcfef7803c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version: 3.12.3 (main, Nov  6 2025, 13:44:16) [GCC 13.3.0]\n",
      "platform.python_version(): 3.12.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"sys.version: {sys.version}\")\n",
    "\n",
    "print(f\"platform.python_version(): {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71669525-1843-4572-814a-10da7b8e4cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('28 Oct 1976', '<1976-10-28>')\n"
     ]
    }
   ],
   "source": [
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug',\n",
    "          'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "\n",
    "def generate_date():\n",
    "    year = random.randint(1950, 2050)\n",
    "\n",
    "    monthIdx = random.randint(1, 12)\n",
    "\n",
    "    match monthIdx:\n",
    "        case 1 | 3 | 5 | 7 | 8 | 10 | 12:\n",
    "            day = random.randint(1, 31)\n",
    "        case 4 | 6 | 9 | 11:\n",
    "            day = random.randint(1, 30)\n",
    "        case 2:\n",
    "            if calendar.isleap(year):\n",
    "                day = random.randint(1, 29)\n",
    "            else:\n",
    "                day = random.randint(1, 28)\n",
    "\n",
    "    # 1. –ü–æ—Ä—è–¥–æ–∫: –ì–æ–¥ - –ú–µ—Å—è—Ü - –î–µ–Ω—å\n",
    "    # 2. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ :02d –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–ª—å –≤ –Ω–∞—á–∞–ª–µ, –µ—Å–ª–∏ —á–∏—Å–ª–æ < 10 (–Ω–∞–ø—Ä–∏–º–µ—Ä, 05)\n",
    "    tgt_str = f'{year}-{monthIdx:02d}-{day:02d}'\n",
    "\n",
    "    # –í—Ö–æ–¥ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å: \"25 Jan 2023\"\n",
    "    src_str = f'{day} {months[monthIdx-1]} {year}'\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å—Ç–∞—Ä—Ç–∞ –∏ —Å—Ç–æ–ø–∞\n",
    "    tgt_str = f'<{tgt_str}>'\n",
    "\n",
    "    return src_str, tgt_str\n",
    "\n",
    "\n",
    "print(generate_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaff16cf-22d2-4ca1-9172-e19bd77d115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char2idx: {' ': 0, '-': 1, '0': 2, '1': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11, '<': 12, '>': 13, 'A': 14, 'B': 15, 'C': 16, 'D': 17, 'E': 18, 'F': 19, 'G': 20, 'H': 21, 'I': 22, 'J': 23, 'K': 24, 'L': 25, 'M': 26, 'N': 27, 'O': 28, 'P': 29, 'Q': 30, 'R': 31, 'S': 32, 'T': 33, 'U': 34, 'V': 35, 'W': 36, 'X': 37, 'Y': 38, 'Z': 39, 'a': 40, 'b': 41, 'c': 42, 'd': 43, 'e': 44, 'f': 45, 'g': 46, 'h': 47, 'i': 48, 'j': 49, 'k': 50, 'l': 51, 'm': 52, 'n': 53, 'o': 54, 'p': 55, 'q': 56, 'r': 57, 's': 58, 't': 59, 'u': 60, 'v': 61, 'w': 62, 'x': 63, 'y': 64, 'z': 65}\n",
      "idx2char: {0: ' ', 1: '-', 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9', 12: '<', 13: '>', 14: 'A', 15: 'B', 16: 'C', 17: 'D', 18: 'E', 19: 'F', 20: 'G', 21: 'H', 22: 'I', 23: 'J', 24: 'K', 25: 'L', 26: 'M', 27: 'N', 28: 'O', 29: 'P', 30: 'Q', 31: 'R', 32: 'S', 33: 'T', 34: 'U', 35: 'V', 36: 'W', 37: 'X', 38: 'Y', 39: 'Z', 40: 'a', 41: 'b', 42: 'c', 43: 'd', 44: 'e', 45: 'f', 46: 'g', 47: 'h', 48: 'i', 49: 'j', 50: 'k', 51: 'l', 52: 'm', 53: 'n', 54: 'o', 55: 'p', 56: 'q', 57: 'r', 58: 's', 59: 't', 60: 'u', 61: 'v', 62: 'w', 63: 'x', 64: 'y', 65: 'z'}\n",
      "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤): 66\n",
      "–ò–Ω–¥–µ–∫—Å –±—É–∫–≤—ã 'A': 14\n",
      "–°–∏–º–≤–æ–ª –ø–æ–¥ –∏–Ω–¥–µ–∫—Å–æ–º 10: 8\n"
     ]
    }
   ],
   "source": [
    "# 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (Alphabet)\n",
    "# set() —É–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "chars = set(\"0123456789 abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ<>-\")\n",
    "\n",
    "\n",
    "# 2. –°–æ–∑–¥–∞–µ–º Map: –°–∏–º–≤–æ–ª -> –ò–Ω–¥–µ–∫—Å (Integer)\n",
    "# sorted() —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ–±—ã –ø–æ—Ä—è–¥–æ–∫ –≤—Å–µ–≥–¥–∞ –±—ã–ª –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º\n",
    "# enumerate() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä—ã (—Å—á–µ—Ç—á–∏–∫, —ç–ª–µ–º–µ–Ω—Ç)\n",
    "char2idx = {char: idx for idx, char in enumerate(sorted(chars))}\n",
    "\n",
    "print(f\"char2idx: {char2idx}\")\n",
    "\n",
    "# 3. –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π Map: –ò–Ω–¥–µ–∫—Å -> –°–∏–º–≤–æ–ª (–¥–ª—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞)\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "print(f\"idx2char: {idx2char}\")\n",
    "\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π —Ç–æ–∫–µ–Ω \"EOS\" (End Of Sentence) –∏–ª–∏ PAD, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "# –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–º–Ω–∏–º —Ä–∞–∑–º–µ—Ä –Ω–∞—à–µ–≥–æ \"–∞–ª—Ñ–∞–≤–∏—Ç–∞\"\n",
    "vocab_size = len(char2idx)\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤): {vocab_size}\")\n",
    "print(f\"–ò–Ω–¥–µ–∫—Å –±—É–∫–≤—ã 'A': {char2idx['A']}\")\n",
    "print(f\"–°–∏–º–≤–æ–ª –ø–æ–¥ –∏–Ω–¥–µ–∫—Å–æ–º 10: {idx2char[10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faafb3de-4775-4951-8f19-b5ef95865d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 1. –°–ª–æ–π Embeddings\n",
    "        # –≠—Ç–æ —Å–ª–æ–≤–∞—Ä—å, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –∏–Ω–¥–µ–∫—Å —Å–∏–º–≤–æ–ª–∞ (int) –≤ –≤–µ–∫—Ç–æ—Ä (float[]).\n",
    "        # input_size: —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–Ω–∞—à vocab_size, –æ–∫–æ–ª–æ 66 —Å–∏–º–≤–æ–ª–æ–≤)\n",
    "        # hidden_size: —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 128 —á–∏—Å–µ–ª –Ω–∞ –æ–¥–Ω—É –±—É–∫–≤—É)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # 2. –°–ª–æ–π GRU (Gated Recurrent Unit)\n",
    "        # –≠—Ç–æ —Å–∞–º–∞ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è \"–ø–∞–º—è—Ç—å\". \n",
    "        # –û–Ω–∞ –±–µ—Ä–µ—Ç –≤–µ–∫—Ç–æ—Ä –±—É–∫–≤—ã –∏ —Å—Ç–∞—Ä—É—é –ø–∞–º—è—Ç—å -> –≤—ã–¥–∞–µ—Ç –Ω–æ–≤—É—é –ø–∞–º—è—Ç—å.\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input: —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –∏–Ω–¥–µ–∫—Å –±—É–∫–≤—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–∏—Å–ª–æ 14)\n",
    "        \n",
    "        # –®–∞–≥ –ê: –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –∏–Ω–¥–µ–∫—Å –≤ –ø–ª–æ—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä\n",
    "        # .view(1, 1, -1) ‚Äî —ç—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –º–æ–º–µ–Ω—Ç PyTorch.\n",
    "        # –ù–µ–π—Ä–æ—Å–µ—Ç—å –∂–¥–µ—Ç 3D-–º–∞—Å—Å–∏–≤: [Batch Size, Sequence Length, Features]\n",
    "        # –ú—ã –≥–æ–≤–æ—Ä–∏–º: \"–£ –Ω–∞—Å 1 –ø–∞–∫–µ—Ç, 1 –±—É–∫–≤–∞ –≤ —Ü–µ–ø–æ—á–∫–µ, –∏ –∞–≤—Ç–æ-—Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ (-1)\"\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        \n",
    "        # –®–∞–≥ –ë: –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ GRU\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—É—Å—Ç–æ–π \"–ø–∞–º—è—Ç–∏\" (–Ω—É–ª–µ–π) –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —á—Ç–µ–Ω–∏—è\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62356363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞: torch.Size([1, 1])\n",
      "–†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–∞ (–ø–∞–º—è—Ç–∏): torch.Size([1, 1, 128])\n",
      "‚úÖ –≠–Ω–∫–æ–¥–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\n"
     ]
    }
   ],
   "source": [
    "# 1. –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –≠–Ω–∫–æ–¥–µ—Ä–∞\n",
    "# vocab_size –º—ã –ø–æ–ª—É—á–∏–ª–∏ –Ω–∞ –ø—Ä–æ—à–ª–æ–º —ç—Ç–∞–ø–µ (–æ–∫–æ–ª–æ 66)\n",
    "# hidden_size = 128 (—Ä–∞–∑–º–µ—Ä –Ω–∞—à–µ–≥–æ \"–º–æ–∑–≥–∞\")\n",
    "test_encoder = EncoderRNN(vocab_size, 128).to(device)\n",
    "\n",
    "# 2. –≠–º—É–ª–∏—Ä—É–µ–º –≤—Ö–æ–¥: –±–µ—Ä–µ–º –∏–Ω–¥–µ–∫—Å –±—É–∫–≤—ã 'A'\n",
    "# tensor([[14]]) ‚Äî —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∏–Ω–¥–µ–∫—Å.\n",
    "# .to(device) –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –µ–≥–æ –Ω–∞ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—É.\n",
    "test_input = torch.tensor([[char2idx['A']]], device=device)\n",
    "\n",
    "# 3. –≠–º—É–ª–∏—Ä—É–µ–º –ø—É—Å—Ç—É—é –ø–∞–º—è—Ç—å\n",
    "test_hidden = test_encoder.initHidden()\n",
    "\n",
    "# 4. –ü—Ä–æ–≥–æ–Ω—è–µ–º (Forward pass)\n",
    "test_output, test_hidden_new = test_encoder(test_input, test_hidden)\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞: {test_input.shape}\") # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å [1, 1]\n",
    "assert test_input.shape == torch.Size([1,1])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–∞ (–ø–∞–º—è—Ç–∏): {test_hidden_new.shape}\") # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å [1, 1, 128]\n",
    "\n",
    "assert test_hidden_new.shape == torch.Size([1, 1, 128])\n",
    "\n",
    "\n",
    "\n",
    "print(\"‚úÖ –≠–Ω–∫–æ–¥–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5555b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length=15):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è)\n",
    "\n",
    "        # 1. –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã)\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        # 2. –°–ª–æ–π –í–Ω–∏–º–∞–Ω–∏—è (–°–∞–º—ã–π –≤–∞–∂–Ω—ã–π!)\n",
    "        # –í—Ö–æ–¥: (–¢–µ–∫—É—â–∏–π —ç–º–µ–¥–¥–∏–Ω–≥ + –°–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ) -> –†–∞–∑–º–µ—Ä = hidden_size * 2\n",
    "        # –í—ã—Ö–æ–¥: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ max_length —à–∞–≥–æ–≤\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "\n",
    "        # 3. –°–ª–æ–π –°–º–µ—à–∏–≤–∞–Ω–∏—è (Combine)\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ—Ç (–¢–µ–∫—É—â–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥ + –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–Ω–∏–º–∞–Ω–∏—è) -> hidden_size\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "        # 4. GRU (–æ–±—ã—á–Ω–∞—è –ø–∞–º—è—Ç—å)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        # 5. –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–ü—Ä–æ–µ–∫—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å)\n",
    "        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä –º—ã—Å–ª–∏ (128) –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –±—É–∫–≤ (66)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input: –ò–Ω–¥–µ–∫—Å –æ–¥–Ω–æ–π –±—É–∫–≤—ã (1, 1)\n",
    "        # hidden: –ü–∞–º—è—Ç—å —Å –ø—Ä–æ—à–ª–æ–≥–æ —à–∞–≥–∞ (1, 1, 128)\n",
    "        # encoder_outputs: –í—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–π —Ñ—Ä–∞–∑—ã (max_length, 128)\n",
    "\n",
    "        embedded = self.embedding(input).view(1, 1, -1) # (1, 1, 128)\n",
    "\n",
    "        # –®–ê–ì –ê: –í—ã—á–∏—Å–ª—è–µ–º Attention Weights\n",
    "        # –°–æ–µ–¥–∏–Ω—è–µ–º (concat) –≤—Ö–æ–¥ –∏ –ø–∞–º—è—Ç—å: [1, 256]\n",
    "        attn_input = torch.cat((embedded[0], hidden[0]), 1)\n",
    "        \n",
    "        # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –∏ Softmax -> –ø–æ–ª—É—á–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã\n",
    "        # –†–µ–∑—É–ª—å—Ç–∞—Ç: [1, 15] (–Ω–∞–ø—Ä–∏–º–µ—Ä: 0.1, 0.8, 0.1 ...)\n",
    "        attn_weights = F.softmax(self.attn(attn_input), dim=1)\n",
    "\n",
    "        # –®–ê–ì –ë: –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–Ω–∏–º–∞–Ω–∏–µ (–ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ)\n",
    "        # (1, 1, 15) x (1, 15, 128) -> (1, 1, 128)\n",
    "        # –ú—ã \"–≤–∑–≤–µ—Å–∏–ª–∏\" –≤—ã—Ö–æ–¥—ã —ç–Ω–∫–æ–¥–µ—Ä–∞.\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # –®–ê–ì –í: –°–∫–∞—Ä–º–ª–∏–≤–∞–µ–º —ç—Ç–æ –≤ GRU\n",
    "        # –°–æ–µ–¥–∏–Ω—è–µ–º —Ç–æ, —á—Ç–æ –ø—Ä–∏—à–ª–æ –Ω–∞ –≤—Ö–æ–¥, —Å —Ç–µ–º, —á—Ç–æ –Ω–∞—à–ª–∏ –≤–Ω–∏–º–∞–Ω–∏–µ–º\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1) # (1, 256)\n",
    "        output = self.attn_combine(output).unsqueeze(0)       # (1, 1, 128)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        # –®–ê–ì –ì: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–π –±—É–∫–≤—ã\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "\n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º attn_weights, —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –∏—Ö –Ω–∞—Ä–∏—Å–æ–≤–∞—Ç—å!\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8893118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –±—É–∫–≤): torch.Size([1, 66])\n",
      "–†–∞–∑–º–µ—Ä –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: torch.Size([1, 15])\n",
      "‚úÖ –î–µ–∫–æ–¥–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\n"
     ]
    }
   ],
   "source": [
    "# 1. –°–æ–∑–¥–∞–µ–º –î–µ–∫–æ–¥–µ—Ä\n",
    "test_decoder = AttnDecoderRNN(hidden_size=128, output_size=vocab_size).to(device)\n",
    "\n",
    "# 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ñ–µ–π–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (—ç–º—É–ª—è—Ü–∏—è —Å–∏—Ç—É–∞—Ü–∏–∏)\n",
    "# –í—Ö–æ–¥: –ò–Ω–¥–µ–∫—Å —Å–∏–º–≤–æ–ª–∞ '<' (–Ω–∞—á–∞–ª–æ —Å—Ç—Ä–æ–∫–∏)\n",
    "test_input = torch.tensor([[char2idx['<']]], device=device)\n",
    "# –ü–∞–º—è—Ç—å: –±–µ—Ä–µ–º –∏–∑ —Ç–µ—Å—Ç–∞ —ç–Ω–∫–æ–¥–µ—Ä–∞ (–∏–ª–∏ –Ω—É–ª–∏)\n",
    "test_hidden = test_encoder.initHidden() # (1, 1, 128)\n",
    "# –í—ã—Ö–æ–¥—ã —ç–Ω–∫–æ–¥–µ—Ä–∞: –≠–º—É–ª–∏—Ä—É–µ–º, –±—É–¥—Ç–æ —ç–Ω–∫–æ–¥–µ—Ä –ø—Ä–æ—á–∏—Ç–∞–ª —Ñ—Ä–∞–∑—É –∏–∑ 15 –±—É–∫–≤\n",
    "# –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É (15, 128)\n",
    "test_encoder_outputs = torch.zeros(15, 128, device=device)\n",
    "\n",
    "# 3. –ü—Ä–æ–≥–æ–Ω—è–µ–º (Forward pass)\n",
    "test_output, test_hidden_new, test_attn = test_decoder(test_input, test_hidden, test_encoder_outputs)\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –±—É–∫–≤): {test_output.shape}\") # (1, 66)\n",
    "assert test_output.shape == torch.Size([1,66])\n",
    "\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: {test_attn.shape}\") # (1, 15) - –≤–∞–∂–Ω–æ!\n",
    "print(\"‚úÖ –î–µ–∫–æ–¥–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\")\n",
    "assert test_attn.shape == torch.Size([1,15])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d83fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–∞—Ä–∫–µ—Ä –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏ (End Of Sentence).\n",
    "# –í –Ω–∞—à–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ –º—ã –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –¥–∞—Ç—É –≤ <...>.\n",
    "# –ó–Ω–∞—á–∏—Ç, —Å–∏–º–≤–æ–ª–æ–º –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –±—É–¥–µ—Ç '>'.\n",
    "EOS_token = char2idx['>']\n",
    "\n",
    "def tensorFromSentence(sentence):\n",
    "    # 1. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "    indexes = [char2idx[char] for char in sentence]\n",
    "    \n",
    "    # 2. –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä –∫–æ–Ω—Ü–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞)\n",
    "    # –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ '>' —É–∂–µ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏ –æ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞, \n",
    "    # –Ω–æ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–Ω —Ç–∞–º –µ—Å—Ç—å.\n",
    "    # –ù–∞—à –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤—ã–¥–∞–µ—Ç '<...>', —Ç–∞–∫ —á—Ç–æ '>' —É–∂–µ –≤–Ω—É—Ç—Ä–∏ indexes.\n",
    "    \n",
    "    # 3. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –¢–µ–Ω–∑–æ—Ä\n",
    "    # dtype=torch.long –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è –∏–Ω–¥–µ–∫—Å–æ–≤ (Embedding —Å–ª–æ–π —Ç—Ä–µ–±—É–µ—Ç long)\n",
    "    # .view(-1, 1) –¥–µ–ª–∞–µ—Ç –∏–∑ –≤–µ–∫—Ç–æ—Ä–∞ —Å—Ç–æ–ª–±–∏–∫ (Sequence Length x Batch Size)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "917216c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_forcing_ratio = 0.5: \n",
    "# –í 50% —Å–ª—É—á–∞–µ–≤ –º—ã —Å–∞–º–∏ –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, \n",
    "# –≤ 50% - –∑–∞—Å—Ç–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –æ–ø–∏—Ä–∞—Ç—å—Å—è –Ω–∞ —Å–≤–æ–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train_step(input_tensor, target_tensor, encoder, decoder, \n",
    "               encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # –ú–∞—Å—Å–∏–≤ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –≤—ã—Ö–æ–¥–æ–≤ —ç–Ω–∫–æ–¥–µ—Ä–∞\n",
    "    encoder_outputs = torch.zeros(decoder.max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    # === –í–ê–ñ–ù–û: –≠—Ç–∞ —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ó–î–ï–°–¨, —Å —Ç–∞–∫–∏–º –∂–µ –æ—Ç—Å—Ç—É–ø–æ–º, –∫–∞–∫ for –Ω–∏–∂–µ ===\n",
    "    # –°–æ–∑–¥–∞–µ–º –Ω—É–ª–µ–≤–æ–π —Ç–µ–Ω–∑–æ—Ä. –≠—Ç–æ –±–∞–∑–∞ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–∫–∏.\n",
    "    loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # === 2. –ó–ê–ü–£–°–ö –≠–ù–ö–û–î–ï–†–ê ===\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # === 3. –ó–ê–ü–£–°–ö –î–ï–ö–û–î–ï–†–ê ===\n",
    "    decoder_input = torch.tensor([[char2idx['<']]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # –†–µ–∂–∏–º –£—á–∏—Ç–µ–ª—è\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di] \n",
    "    else:\n",
    "        # –†–µ–∂–∏–º –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.detach()\n",
    "            if decoder_input.item() == char2idx['>']:\n",
    "                break\n",
    "\n",
    "    # === 4. –û–ë–£–ß–ï–ù–ò–ï ===\n",
    "    # Pylance —Ä—É–≥–∞–ª—Å—è –∑–¥–µ—Å—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –±–æ—è–ª—Å—è, —á—Ç–æ loss –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω.\n",
    "    # –ù–æ —Ç–µ–ø–µ—Ä—å –æ–Ω —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –Ω–∞—á–∞–ª–µ —Ñ—É–Ω–∫—Ü–∏–∏.\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6887b28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 10000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä...\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ!\n",
      "üìö –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ (Train): 8000 —à—Ç.\n",
      "üéì –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ (Test):  2000 —à—Ç. (–ú–æ–¥–µ–ª—å –∏—Ö –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —É–≤–∏–¥–∏—Ç)\n",
      "–ü—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ—Å—Ç–∞: ('12 Nov 1965', '<1965-11-12>')\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "TOTAL_SAMPLES = 10000  # –°–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ö–æ—Ç–∏–º\n",
    "TRAIN_RATIO = 0.8      # 80% –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "# 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—É–ª –¥–∞–Ω–Ω—ã—Ö\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º set, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –¥–∞—Ç—ã)\n",
    "dataset_set = set()\n",
    "\n",
    "print(f\"üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {TOTAL_SAMPLES} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä...\")\n",
    "\n",
    "while len(dataset_set) < TOTAL_SAMPLES:\n",
    "    # –í—ã–∑—ã–≤–∞–µ–º —Ç–≤–æ—é —Ñ—É–Ω–∫—Ü–∏—é (–æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –≤—ã—à–µ)\n",
    "    pair = generate_date() \n",
    "    dataset_set.add(pair)\n",
    "\n",
    "# –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º\n",
    "full_dataset = list(dataset_set)\n",
    "random.shuffle(full_dataset)\n",
    "\n",
    "# 2. –†–∞–∑—Ä–µ–∑–∞–µ–º –Ω–∞ Train –∏ Test\n",
    "split_index = int(TOTAL_SAMPLES * TRAIN_RATIO)\n",
    "\n",
    "train_data = full_dataset[:split_index]\n",
    "test_data = full_dataset[split_index:]\n",
    "\n",
    "print(f\"‚úÖ –ì–æ—Ç–æ–≤–æ!\")\n",
    "print(f\"üìö –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ (Train): {len(train_data)} —à—Ç.\")\n",
    "print(f\"üéì –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ (Test):  {len(test_data)} —à—Ç. (–ú–æ–¥–µ–ª—å –∏—Ö –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —É–≤–∏–¥–∏—Ç)\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ—Å—Ç–∞: {test_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c7bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ß–ï–°–¢–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 8000 –ø—Ä–∏–º–µ—Ä–∞—Ö...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "hidden_size = 128\n",
    "n_iters = 20000  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
    "learning_rate = 0.01\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –∑–∞–Ω–æ–≤–æ (—á—Ç–æ–±—ã —Å—Ç–µ—Ä–µ—Ç—å —Å—Ç–∞—Ä—É—é –ø–∞–º—è—Ç—å/–≤–µ—Å–∞)\n",
    "encoder = EncoderRNN(vocab_size, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, vocab_size).to(device)\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "plot_losses = []\n",
    "print_every = 1000\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "print(f\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ß–ï–°–¢–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(train_data)} –ø—Ä–∏–º–µ—Ä–∞—Ö...\")\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    # –í–ê–ñ–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï:\n",
    "    # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é –ø–∞—Ä—É –¢–û–õ–¨–ö–û –∏–∑ train_data\n",
    "    # (random.choice –≤—ã–±–∏—Ä–∞–µ—Ç –æ–¥–∏–Ω —Å–ª—É—á–∞–π–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –∏–∑ —Å–ø–∏—Å–∫–∞)\n",
    "    training_pair = random.choice(train_data)\n",
    "    src_str = training_pair[0]\n",
    "    tgt_str = training_pair[1]\n",
    "    \n",
    "    # –î–∞–ª—å—à–µ –≤—Å—ë –∫–∞–∫ —Ä–∞–Ω—å—à–µ\n",
    "    input_tensor = tensorFromSentence(src_str)\n",
    "    target_tensor = tensorFromSentence(tgt_str)\n",
    "\n",
    "    loss = train_step(input_tensor, target_tensor, encoder, decoder,\n",
    "                     encoder_optimizer, decoder_optimizer, criterion)\n",
    "    \n",
    "    current_loss += loss\n",
    "    all_losses.append(loss)\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print_loss_avg = current_loss / print_every\n",
    "        current_loss = 0\n",
    "        print(f'{iter} ({iter / n_iters * 100:.0f}%) time: {time.time()-start:.2f}s | loss: {print_loss_avg:.4f}')\n",
    "\n",
    "print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ab308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (—Å–≤–µ—Ä—Ç–∫–∞)\n",
    "def moving_average(data, window_size=50):\n",
    "    # window_size=50 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–∞–∂–¥–∞—è —Ç–æ—á–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞ - —ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –∑–∞ 50 —à–∞–≥–æ–≤\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –∫ —Ç–≤–æ–µ–º—É —Å–ø–∏—Å–∫—É –æ—à–∏–±–æ–∫\n",
    "smooth_losses = moving_average(all_losses)\n",
    "\n",
    "# –†–∏—Å—É–µ–º\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(smooth_losses, label='Smoothed Loss')\n",
    "plt.title(\"–ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è (–°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ)\")\n",
    "plt.xlabel(\"–ò—Ç–µ—Ä–∞—Ü–∏–∏\")\n",
    "plt.ylabel(\"–û—à–∏–±–∫–∞\")\n",
    "plt.grid(True, alpha=0.3) # –î–æ–±–∞–≤–∏–º —Å–µ—Ç–∫—É –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    with torch.no_grad(): # –û—Ç–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞\n",
    "        input_tensor = tensorFromSentence(input_sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        # –ú–∞—Å—Å–∏–≤ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (–¥–ª—è –∫–∞—Ä—Ç–∏–Ω–∫–∏)\n",
    "        decoder_attentions = torch.zeros(decoder.max_length, decoder.max_length)\n",
    "\n",
    "        encoder_outputs = torch.zeros(decoder.max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        # 1. –ß–∏—Ç–∞–µ–º (Encoder)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        # 2. –ü–∏—à–µ–º (Decoder)\n",
    "        decoder_input = torch.tensor([[char2idx['<']]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(decoder.max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            \n",
    "            if topi.item() == char2idx['>']:\n",
    "                break # –ö–æ–Ω–µ—Ü —Ñ—Ä–∞–∑—ã\n",
    "            else:\n",
    "                decoded_words.append(idx2char[topi.item()])\n",
    "\n",
    "            decoder_input = topi.detach()\n",
    "\n",
    "        return \"\".join(decoded_words), decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly(n=5):\n",
    "    print(f\"\\nüéì –≠–ö–ó–ê–ú–ï–ù (–î–∞–Ω–Ω—ã–µ –∏–∑ Test Set):\")\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        # –ë–µ—Ä–µ–º –∏–∑ TEST data\n",
    "        pair = random.choice(test_data)\n",
    "        src = pair[0]\n",
    "        tgt_real = pair[1]\n",
    "        \n",
    "        # –ü—Ä–æ—Å–∏–º –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å\n",
    "        output_words, _ = evaluate(src) # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é evaluate\n",
    "        \n",
    "        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º (—É–±–∏—Ä–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã < > –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã)\n",
    "        tgt_clean = tgt_real.replace('<','').replace('>','')\n",
    "        out_clean = output_words.replace('<','').replace('>','')\n",
    "        \n",
    "        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ\n",
    "        is_match = \"‚úÖ\" if tgt_clean == out_clean else f\"‚ùå (–û–∂–∏–¥–∞–ª–æ—Å—å: {tgt_clean})\"\n",
    "        if tgt_clean == out_clean: correct += 1\n",
    "            \n",
    "        print(f\"–í—Ö–æ–¥: {src.ljust(15)} -> –í—ã—Ö–æ–¥: {out_clean} {is_match}\")\n",
    "        \n",
    "    print(f\"\\n–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã–±–æ—Ä–∫–µ: {correct}/{n}\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫–∑–∞–º–µ–Ω\n",
    "evaluate_randomly(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    \n",
    "    # –†–∏—Å—É–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # –ü–æ–¥–ø–∏—Å–∏ –æ—Å–µ–π\n",
    "    ax.set_xticklabels([''] + list(input_sentence), rotation=90)\n",
    "    ax.set_yticklabels([''] + list(output_words))\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    print(f\"–í—Ö–æ–¥:  {input_sentence}\")\n",
    "    print(f\"–í—ã—Ö–æ–¥: {output_words}\")\n",
    "\n",
    "# === –ó–ê–ü–£–°–ö –ü–†–û–í–ï–†–ö–ò ===\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥–∞—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª—å –ù–ï –≤–∏–¥–µ–ª–∞\n",
    "def showAttentions(n = 3):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(test_data)\n",
    "        src = pair[0]\n",
    "        showAttention(src)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
