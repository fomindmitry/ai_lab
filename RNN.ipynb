{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd035308-996c-44cc-bda3-37681585fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import calendar\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU (–∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ —Ä–∞–Ω—å—à–µ)\n",
    "# –ï—Å–ª–∏ CUDA –µ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë. –ï—Å–ª–∏ –Ω–µ—Ç - –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä.\n",
    "\n",
    "print(f\"torch.version.cuda: {torch.version.cuda}\")       # –î–æ–ª–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å '12.4' –∏–ª–∏ '12.1' (–Ω–µ None!)\n",
    "print(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\") # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å True\n",
    "print(f\"torch.xpu.is_available(): {torch.xpu.is_available()}\") \n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ç—Ä–∏–±—É—Ç xpu, —á—Ç–æ–±—ã –∫–æ–¥ –Ω–µ –ø–∞–¥–∞–ª –Ω–∞ –º–∞—à–∏–Ω–∞—Ö —Å–æ —Å—Ç–∞—Ä—ã–º PyTorch\n",
    "    elif hasattr(torch, 'xpu') and torch.xpu.is_available():\n",
    "        return torch.device(\"xpu\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "print(f\"‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: {device}\")\n",
    "# –ï—Å–ª–∏ XPU –∞–∫—Ç–∏–≤–µ–Ω, –ø–æ–ª–µ–∑–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ –∏–º—è –∫–∞—Ä—Ç—ã:\n",
    "if device.type == 'xpu':\n",
    "    print(f\"   –ö–∞—Ä—Ç–∞: {torch.xpu.get_device_name(0)}\")\n",
    "elif device.type == 'cuda':\n",
    "    print(f\"   –ö–∞—Ä—Ç–∞: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfafb8-4f0a-4b8b-b468-dcfef7803c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sys.version: {sys.version}\")\n",
    "\n",
    "print(f\"platform.python_version(): {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71669525-1843-4572-814a-10da7b8e4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug',\n",
    "          'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "\n",
    "def generate_date():\n",
    "    year = random.randint(1000, 2100)\n",
    "\n",
    "    monthIdx = random.randint(1, 12)\n",
    "\n",
    "    match monthIdx:\n",
    "        case 1 | 3 | 5 | 7 | 8 | 10 | 12:\n",
    "            day = random.randint(1, 31)\n",
    "        case 4 | 6 | 9 | 11:\n",
    "            day = random.randint(1, 30)\n",
    "        case 2:\n",
    "            if calendar.isleap(year):\n",
    "                day = random.randint(1, 29)\n",
    "            else:\n",
    "                day = random.randint(1, 28)\n",
    "\n",
    "    # 1. –ü–æ—Ä—è–¥–æ–∫: –ì–æ–¥ - –ú–µ—Å—è—Ü - –î–µ–Ω—å\n",
    "    # 2. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ :02d –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–ª—å –≤ –Ω–∞—á–∞–ª–µ, –µ—Å–ª–∏ —á–∏—Å–ª–æ < 10 (–Ω–∞–ø—Ä–∏–º–µ—Ä, 05)\n",
    "    tgt_str = f'{year}-{monthIdx:02d}-{day:02d}'\n",
    "\n",
    "    # –í—Ö–æ–¥ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å: \"25 Jan 2023\"\n",
    "    src_str = f'{day} {months[monthIdx-1]} {year}'\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å—Ç–∞—Ä—Ç–∞ –∏ —Å—Ç–æ–ø–∞\n",
    "    tgt_str = f'<{tgt_str}>'\n",
    "\n",
    "    return src_str, tgt_str\n",
    "\n",
    "\n",
    "print(generate_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SAMPLES = 50000\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "print(f\"üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {TOTAL_SAMPLES} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–≥–æ–¥—ã 1000-2100)...\")\n",
    "\n",
    "dataset_set = set()\n",
    "\n",
    "# –¢–µ–ø–µ—Ä—å —ç—Ç–æ—Ç —Ü–∏–∫–ª –Ω–µ –∑–∞–≤–∏—Å–Ω–µ—Ç, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ (400k) > —Ü–µ–ª–∏ (50k)\n",
    "while len(dataset_set) < TOTAL_SAMPLES:\n",
    "    dataset_set.add(generate_date())\n",
    "\n",
    "full_dataset = list(dataset_set)\n",
    "random.shuffle(full_dataset)\n",
    "\n",
    "split_index = int(TOTAL_SAMPLES * TRAIN_RATIO)\n",
    "train_data = full_dataset[:split_index]\n",
    "test_data = full_dataset[split_index:]\n",
    "\n",
    "print(f\"‚úÖ –ì–æ—Ç–æ–≤–æ! Train: {len(train_data)}, Test: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff16cf-22d2-4ca1-9172-e19bd77d115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã (–∑–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º –∏—Ö –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞)\n",
    "# –í–∞–∂–Ω–æ: 0-–π –∏–Ω–¥–µ–∫—Å —á–∞—Å—Ç–æ —Ä–µ–∑–µ—Ä–≤–∏—Ä—É—é—Ç –¥–ª—è PAD, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "# –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω—É–ª—è–º–∏.\n",
    "PAD_token = 0\n",
    "SOS_token = 1 \n",
    "EOS_token = 2\n",
    "\n",
    "# –°–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –±—É–¥—É—Ç –≤ –Ω–∞—à–µ–º —Å–ª–æ–≤–∞—Ä–µ\n",
    "special_chars = {\n",
    "    '<PAD>': PAD_token, \n",
    "    '<SOS>': SOS_token, \n",
    "    '<EOS>': EOS_token\n",
    "}\n",
    "\n",
    "\n",
    "# 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (Alphabet)\n",
    "# set() —É–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "chars = set(\"0123456789 abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ<>-\")\n",
    "\n",
    "# 2. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–∏–º–≤–æ–ª—ã –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è char2idx\n",
    "# –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å 3 (–ø–æ—Å–ª–µ 0, 1, 2)\n",
    "sorted_data_chars = sorted(chars)\n",
    "next_index = len(special_chars)\n",
    "\n",
    "# 3. –°–æ–∑–¥–∞–µ–º Map: –°–∏–º–≤–æ–ª -> –ò–Ω–¥–µ–∫—Å (Integer)\n",
    "# sorted() —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ–±—ã –ø–æ—Ä—è–¥–æ–∫ –≤—Å–µ–≥–¥–∞ –±—ã–ª –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º\n",
    "# enumerate() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä—ã (—Å—á–µ—Ç—á–∏–∫, —ç–ª–µ–º–µ–Ω—Ç)\n",
    "char2idx = special_chars.copy()\n",
    "char2idx.update({char: idx + next_index for idx, char in enumerate(sorted_data_chars)})\n",
    "print(f\"char2idx: {char2idx}\")\n",
    "\n",
    "# 3. –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π Map: –ò–Ω–¥–µ–∫—Å -> –°–∏–º–≤–æ–ª (–¥–ª—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞)\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "print(f\"idx2char: {idx2char}\")\n",
    "\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π —Ç–æ–∫–µ–Ω \"EOS\" (End Of Sentence) –∏–ª–∏ PAD, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "# –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–º–Ω–∏–º —Ä–∞–∑–º–µ—Ä –Ω–∞—à–µ–≥–æ \"–∞–ª—Ñ–∞–≤–∏—Ç–∞\"\n",
    "vocab_size = len(char2idx)\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤): {vocab_size}\")\n",
    "print(f\"–ò–Ω–¥–µ–∫—Å –±—É–∫–≤—ã 'A': {char2idx['A']}\")\n",
    "print(f\"–°–∏–º–≤–æ–ª –ø–æ–¥ –∏–Ω–¥–µ–∫—Å–æ–º 10: {idx2char[10]}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Å —Ç–æ–∫–µ–Ω–∞–º–∏: {vocab_size}\")\n",
    "print(f\"–ò–Ω–¥–µ–∫—Å <PAD>: {char2idx['<PAD>']}\")\n",
    "print(f\"–ò–Ω–¥–µ–∫—Å <EOS>: {char2idx['<EOS>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafb3de-4775-4951-8f19-b5ef95865d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class EncoderRNN_Packed(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN_Packed, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    # –î–æ–±–∞–≤–∏–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç lengths (–¥–ª–∏–Ω—ã —Ñ—Ä–∞–∑ –≤ –±–∞—Ç—á–µ)\n",
    "    def forward(self, input, lengths, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # 1. –£–ü–ê–ö–û–í–ö–ê (PACKING)\n",
    "        # –ú—ã –≥–æ–≤–æ—Ä–∏–º RNN: \"–ò–≥–Ω–æ—Ä–∏—Ä—É–π —Ö–≤–æ—Å—Ç—ã!\"\n",
    "        # enforce_sorted=False –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –±–∞—Ç—á –ø–æ –¥–ª–∏–Ω–µ (—É–¥–æ–±–Ω–æ!)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), enforce_sorted=False)\n",
    "        \n",
    "        # 2. –ü–†–û–ì–û–ù (—É–∂–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)\n",
    "        output, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        # 3. –†–ê–°–ü–ê–ö–û–í–ö–ê (UNPACKING)\n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å—ë –≤ –ø—Ä–∏–≤—ã—á–Ω—ã–π –≤–∏–¥ (—Å –Ω—É–ª—è–º–∏), —á—Ç–æ–±—ã –î–µ–∫–æ–¥–µ—Ä –º–æ–≥ —Ä–∞–±–æ—Ç–∞—Ç—å\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "print(\"‚úÖ Packed Encoder –≥–æ—Ç–æ–≤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5555b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AttnDecoderRNN_Masked(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length=15, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN_Masked, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    # –î–æ–±–∞–≤–∏–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç encoder_mask\n",
    "    def forward(self, input, hidden, encoder_outputs, encoder_mask):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_input = torch.cat((embedded[0], hidden[0]), 1) \n",
    "        \n",
    "        # 1. –°—á–∏—Ç–∞–µ–º \"—Å—ã—Ä—ã–µ\" —ç–Ω–µ—Ä–≥–∏–∏ (–¥–æ Softmax)\n",
    "        attn_energies = self.attn(attn_input) # (Batch, Max_Len)\n",
    "\n",
    "        # 2. –ü–†–ò–ú–ï–ù–Ø–ï–ú –ú–ê–°–ö–£!\n",
    "        if encoder_mask is not None:\n",
    "            # –¢–∞–º, –≥–¥–µ –º–∞—Å–∫–∞ == 0 (PAD), —Å—Ç–∞–≤–∏–º -1 –º–∏–ª–ª–∏–∞—Ä–¥.\n",
    "            # Softmax(-1e9) -> 0.0\n",
    "            attn_energies = attn_energies.masked_fill(encoder_mask == 0, -1e9)\n",
    "\n",
    "        # 3. –¢–µ–ø–µ—Ä—å –±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–µ–ª–∞–µ–º Softmax\n",
    "        attn_weights = F.softmax(attn_energies, dim=1)\n",
    "\n",
    "        encoder_outputs_transposed = encoder_outputs.transpose(0, 1)\n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n",
    "                                 encoder_outputs_transposed)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[:, 0, :]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0) \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "print(\"‚úÖ Masked Decoder –≥–æ—Ç–æ–≤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d83fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–∞—Ä–∫–µ—Ä –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏ (End Of Sentence).\n",
    "# –í –Ω–∞—à–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ –º—ã –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –¥–∞—Ç—É –≤ <...>.\n",
    "# –ó–Ω–∞—á–∏—Ç, —Å–∏–º–≤–æ–ª–æ–º –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –±—É–¥–µ—Ç '>'.\n",
    "EOS_token = char2idx['>']\n",
    "\n",
    "def tensorFromSentence(sentence):\n",
    "    # 1. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "    indexes = [char2idx[char] for char in sentence]\n",
    "    \n",
    "    # 2. –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä –∫–æ–Ω—Ü–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞)\n",
    "    # –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ '>' —É–∂–µ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏ –æ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞, \n",
    "    # –Ω–æ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–Ω —Ç–∞–º –µ—Å—Ç—å.\n",
    "    # –ù–∞—à –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤—ã–¥–∞–µ—Ç '<...>', —Ç–∞–∫ —á—Ç–æ '>' —É–∂–µ –≤–Ω—É—Ç—Ä–∏ indexes.\n",
    "    \n",
    "    # 3. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –¢–µ–Ω–∑–æ—Ä\n",
    "    # dtype=torch.long –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è –∏–Ω–¥–µ–∫—Å–æ–≤ (Embedding —Å–ª–æ–π —Ç—Ä–µ–±—É–µ—Ç long)\n",
    "    # .view(-1, 1) –¥–µ–ª–∞–µ—Ç –∏–∑ –≤–µ–∫—Ç–æ—Ä–∞ —Å—Ç–æ–ª–±–∏–∫ (Sequence Length x Batch Size)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917216c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch_packed(input_batch, target_batch, encoder, decoder, \n",
    "                       encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    batch_size = input_batch.size(1)\n",
    "    target_len = target_batch.size(0)\n",
    "\n",
    "    # --- –°–ß–ò–¢–ê–ï–ú –î–õ–ò–ù–´ ---\n",
    "    # input_batch: (Seq_Len, Batch). –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º -> (Batch, Seq_Len)\n",
    "    # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ –ù–ï-–Ω—É–ª–µ–π –≤ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ\n",
    "    # .cpu() –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è pack_padded_sequence –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö\n",
    "    lengths = (input_batch.transpose(0, 1) != PAD_token).sum(dim=1).cpu()\n",
    "    \n",
    "    # ---------------------\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    \n",
    "    # –ü–ï–†–ï–î–ê–ï–ú –î–õ–ò–ù–´ –í –≠–ù–ö–û–î–ï–†!\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batch, lengths, encoder_hidden)\n",
    "    \n",
    "    # –ú–∞—Å–∫–∞ –¥–ª—è Attention (—Ç–∞ –∂–µ –ª–æ–≥–∏–∫–∞, —á—Ç–æ –∏ —Ä–∞–Ω—å—à–µ)\n",
    "    mask = (input_batch.transpose(0, 1) != PAD_token)\n",
    "    full_mask = torch.zeros(batch_size, decoder.max_length, dtype=torch.bool, device=device)\n",
    "    actual_len = mask.size(1)\n",
    "    limit_len = min(actual_len, decoder.max_length)\n",
    "    full_mask[:, :limit_len] = mask[:, :limit_len]\n",
    "    \n",
    "    # Pad encoder outputs\n",
    "    proj_encoder_outputs = torch.zeros(decoder.max_length, batch_size, encoder.hidden_size, device=device)\n",
    "    # output –æ—Ç pad_packed_sequence –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ—Ä–æ—á–µ max_length, –µ—Å–ª–∏ –≤—Å–µ —Ñ—Ä–∞–∑—ã –∫–æ—Ä–æ—Ç–∫–∏–µ\n",
    "    out_len = encoder_outputs.size(0) \n",
    "    limit_copy = min(out_len, decoder.max_length)\n",
    "    proj_encoder_outputs[:limit_copy, :, :] = encoder_outputs[:limit_copy, :, :]\n",
    "\n",
    "    # –î–µ–∫–æ–¥–µ—Ä (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n",
    "    decoder_input = torch.tensor([[SOS_token] * batch_size], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_len):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, proj_encoder_outputs, encoder_mask=full_mask)\n",
    "            loss += criterion(decoder_output, target_batch[di])\n",
    "            decoder_input = target_batch[di].unsqueeze(0) \n",
    "    else:\n",
    "        for di in range(target_len):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, proj_encoder_outputs, encoder_mask=full_mask)\n",
    "            loss += criterion(decoder_output, target_batch[di])\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.transpose(0, 1).detach()\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), 5.0)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), 5.0)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –°—Ç—Ä–æ–∫–∞ -> –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "def indexesFromSentence(sentence):\n",
    "    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –º–∞—Ä–∫–µ—Ä—ã < –∏ >, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å, —á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞—Ç—å—Å—è\n",
    "    clean_sentence = sentence.replace('<', '').replace('>', '')\n",
    "    \n",
    "    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ —Ü–∏—Ñ—Ä—ã\n",
    "    # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –≤—Å—Ç—Ä–µ—Ç–∏–º –Ω–µ–∑–Ω–∞–∫–æ–º—ã–π —Å–∏–º–≤–æ–ª, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ (–∏–ª–∏ –ø–∞–¥–∞–µ–º, –Ω–æ –ø–æ–∫–∞ –ø—Ä–æ–ø—É—Å—Ç–∏–º)\n",
    "    return [char2idx[char] for char in clean_sentence if char in char2idx] + [EOS_token]\n",
    "\n",
    "def get_batch(batch_size=32):\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    # 1. –ù–∞–±–∏—Ä–∞–µ–º batch_size —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "    for _ in range(batch_size):\n",
    "        pair = random.choice(train_data) # –ë–µ—Ä–µ–º –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "        input_str, target_str = pair\n",
    "        \n",
    "        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ —Å–ø–∏—Å–∫–∏ —á–∏—Å–µ–ª (–ù–ï —Ç–µ–Ω–∑–æ—Ä—ã –ø–æ–∫–∞)\n",
    "        input_idxs = indexesFromSentence(input_str)\n",
    "        target_idxs = indexesFromSentence(target_str)\n",
    "        \n",
    "        # –í–∞–∂–Ω–æ: –¥–ª—è PyTorch –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–∫–∏ –≤ —Ç–µ–Ω–∑–æ—Ä—ã\n",
    "        input_list.append(torch.tensor(input_idxs, dtype=torch.long, device=device))\n",
    "        target_list.append(torch.tensor(target_idxs, dtype=torch.long, device=device))\n",
    "        \n",
    "    # 2. –ú–∞–≥–∏—è PAD_SEQUENCE\n",
    "    # –û–Ω–∞ –±–µ—Ä–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä–æ–≤ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –¥–µ–ª–∞–µ—Ç –∏–∑ –Ω–∏—Ö –æ–¥–∏–Ω –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä.\n",
    "    # padding_value=PAD_token (–Ω–∞—à 0) ‚Äî —á–µ–º –∑–∞–ø–æ–ª–Ω—è—Ç—å –ø—É—Å—Ç–æ—Ç—É.\n",
    "    # –í–ê–ñ–ù–û: –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–Ω–∞ –¥–µ–ª–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (Max_Length, Batch_Size).\n",
    "    # –≠—Ç–æ —Ç–æ, —á—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è RNN (–≤—Ä–µ–º—è –∏–¥–µ—Ç –≤–Ω–∏–∑, –±–∞—Ç—á–∏ –∏–¥—É—Ç –≤–ø—Ä–∞–≤–æ).\n",
    "    input_batch = pad_sequence(input_list, padding_value=PAD_token)\n",
    "    target_batch = pad_sequence(target_list, padding_value=PAD_token)\n",
    "    \n",
    "    return input_batch, target_batch\n",
    "\n",
    "# === –¢–ï–°–¢ ===\n",
    "# –î–∞–≤–∞–π –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –æ–¥–∏–Ω –±–∞—Ç—á\n",
    "inp, tgt = get_batch(batch_size=3)\n",
    "print(f\"–§–æ—Ä–º–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ (Seq_Len, Batch): {inp.shape}\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ü–µ–ª–µ–≤–æ–≥–æ –±–∞—Ç—á–∞ (Seq_Len, Batch): {tgt.shape}\")\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ (–≤–∏–¥–∏–º –Ω—É–ª–∏-–ø–∞–¥–¥–∏–Ω–≥–∏ –≤ –∫–æ–Ω—Ü–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑?):\")\n",
    "print(inp.transpose(0, 1)) # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ (Batch, Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === –≠–¢–ê–ü –ü–†–ï–î-–ó–ê–ì–†–£–ó–ö–ò (ETL) ===\n",
    "# –ú—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –æ–¥–∏–Ω —Ä–∞–∑ –∏ –Ω–∞–≤—Å–µ–≥–¥–∞, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å CPU –≤ —Ü–∏–∫–ª–µ.\n",
    "\n",
    "print(\"üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ VRAM...\")\n",
    "\n",
    "def prepare_full_dataset_tensor(data_list):\n",
    "    input_tensors = []\n",
    "    target_tensors = []\n",
    "    \n",
    "    for src, tgt in data_list:\n",
    "        input_tensors.append(torch.tensor(indexesFromSentence(src), dtype=torch.long))\n",
    "        target_tensors.append(torch.tensor(indexesFromSentence(tgt), dtype=torch.long))\n",
    "    \n",
    "    # –î–µ–ª–∞–µ–º –æ–¥–∏–Ω –≥–∏–≥–∞–Ω—Ç—Å–∫–∏–π Pad –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    # (Total_Samples, Max_Len)\n",
    "    # –í–ê–ñ–ù–û: pad_sequence –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–µ–ª–∞–µ—Ç (Len, Batch).\n",
    "    # –ù–∞–º —É–¥–æ–±–Ω–µ–µ (Batch, Len) –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è, –∞ –≤ —Ü–∏–∫–ª–µ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º.\n",
    "    input_full = pad_sequence(input_tensors, batch_first=True, padding_value=PAD_token)\n",
    "    target_full = pad_sequence(target_tensors, batch_first=True, padding_value=PAD_token)\n",
    "    \n",
    "    return input_full.to(device), target_full.to(device)\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É –Ω–∞ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—É\n",
    "train_input_gpu, train_target_gpu = prepare_full_dataset_tensor(train_data)\n",
    "\n",
    "print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ VRAM GPU!\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞: {train_input_gpu.shape} (–ü—Ä–∏–º–µ—Ä–æ–≤, –î–ª–∏–Ω–∞)\")\n",
    "print(f\"–ó–∞–Ω–∏–º–∞–µ—Ç –ø–∞–º—è—Ç–∏: {train_input_gpu.element_size() * train_input_gpu.numel() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# === CONFIG ===\n",
    "BATCH_SIZE = 64\n",
    "n_iters = 30000  # –ü–æ–ø—Ä–æ–±—É–µ–º 10k —à–∞–≥–æ–≤. –° –º–∞—Å–∫–æ–π –¥–æ–ª–∂–Ω–æ —Ö–≤–∞—Ç–∏—Ç—å.\n",
    "learning_rate = 0.001\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (–ù–æ–≤—ã–π –¥–µ–∫–æ–¥–µ—Ä!)\n",
    "encoder_batch = EncoderRNN_Packed(vocab_size, 128).to(device)\n",
    "decoder_batch = AttnDecoderRNN_Masked(128, vocab_size, max_length=15, dropout_p=0.1).to(device)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "encoder_batch.compile();\n",
    "decoder_batch.compile();\n",
    "\n",
    "enc_optim = optim.Adam(encoder_batch.parameters(), lr=learning_rate)\n",
    "dec_optim = optim.Adam(decoder_batch.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_token)\n",
    "\n",
    "plot_losses = []\n",
    "start = time.time()\n",
    "\n",
    "print(f\"üöÄ –ó–∞–ø—É—Å–∫ MASKED –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    indices = torch.randint(0, len(train_input_gpu), (BATCH_SIZE,), device=device)\n",
    "    input_batch = train_input_gpu[indices].transpose(0, 1)\n",
    "    target_batch = train_target_gpu[indices].transpose(0, 1)\n",
    "    \n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é train_batch_packed\n",
    "    loss = train_batch_packed(input_batch, target_batch, encoder_batch, decoder_batch,\n",
    "                       enc_optim, dec_optim, criterion)\n",
    "    \n",
    "    plot_losses.append(loss)\n",
    "\n",
    "    if iter % 1000 == 0:\n",
    "        speed = (iter * BATCH_SIZE) / (time.time() - start)\n",
    "        print(f'{iter} | Loss: {loss:.5f} | Speed: {speed:.0f} samples/sec')\n",
    "\n",
    "print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ab308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (—Å–≤–µ—Ä—Ç–∫–∞)\n",
    "def moving_average(data, window_size=50):\n",
    "    # window_size=50 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–∞–∂–¥–∞—è —Ç–æ—á–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞ - —ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –∑–∞ 50 —à–∞–≥–æ–≤\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –∫ —Ç–≤–æ–µ–º—É —Å–ø–∏—Å–∫—É –æ—à–∏–±–æ–∫\n",
    "smooth_losses = moving_average(plot_losses)\n",
    "\n",
    "# –†–∏—Å—É–µ–º\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(smooth_losses, label='Smoothed Loss')\n",
    "plt.title(\"–ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è (–°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ)\")\n",
    "plt.xlabel(\"–ò—Ç–µ—Ä–∞—Ü–∏–∏\")\n",
    "plt.ylabel(\"–û—à–∏–±–∫–∞\")\n",
    "plt.grid(True, alpha=0.3) # –î–æ–±–∞–≤–∏–º —Å–µ—Ç–∫—É –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# === –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø (–° –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–∞—Å–æ–∫) ===\n",
    "\n",
    "def evaluate(input_sentence):\n",
    "    encoder_batch.eval()\n",
    "    decoder_batch.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_sentence)\n",
    "        # –î–ª–∏–Ω–∞ –≤—Å–µ–≥–¥–∞ —Ä–∞–≤–Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω–µ —Ç–µ–Ω–∑–æ—Ä–∞\n",
    "        length = torch.tensor([input_tensor.size(0)]) \n",
    "        \n",
    "        encoder_hidden = encoder_batch.initHidden(batch_size=1)\n",
    "        \n",
    "        # –ü–µ—Ä–µ–¥–∞–µ–º length!\n",
    "        encoder_outputs, encoder_hidden = encoder_batch(input_tensor, length, encoder_hidden)\n",
    "        \n",
    "        # ... (–æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...\n",
    "        # (–ù–µ –∑–∞–±—É–¥—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –æ—Å—Ç–∞—Ç–æ–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –ø—Ä–æ—à–ª–æ–π –≤–µ—Ä—Å–∏–∏, –≥–¥–µ –º—ã –¥–µ–ª–∞–ª–∏ –º–∞—Å–∫–∏)\n",
    "        # –î–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏: –∏—Å–ø–æ–ª—å–∑—É–π —Ç—É –∂–µ –ª–æ–≥–∏–∫—É proj_encoder_outputs –∏ encoder_mask, —á—Ç–æ –±—ã–ª–∞\n",
    "        \n",
    "        # --- (–ö–æ–ø–∏–ø–∞—Å—Ç —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞ –Ω–∏–∂–µ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã) ---\n",
    "        proj_encoder_outputs = torch.zeros(decoder_batch.max_length, 1, encoder_batch.hidden_size, device=device)\n",
    "        out_len = encoder_outputs.size(0)\n",
    "        limit_copy = min(out_len, decoder_batch.max_length)\n",
    "        proj_encoder_outputs[:limit_copy, :, :] = encoder_outputs[:limit_copy, :, :]\n",
    "\n",
    "        encoder_mask = torch.zeros(1, decoder_batch.max_length, dtype=torch.bool, device=device)\n",
    "        encoder_mask[:, :limit_copy] = True\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(decoder_batch.max_length, decoder_batch.max_length)\n",
    "\n",
    "        for di in range(decoder_batch.max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder_batch(\n",
    "                decoder_input, decoder_hidden, proj_encoder_outputs, encoder_mask=encoder_mask)\n",
    "            decoder_attentions[di] = decoder_attention.data[0]\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(idx2char[topi.item()])\n",
    "            decoder_input = topi.detach().transpose(0, 1)\n",
    "            \n",
    "    encoder_batch.train()\n",
    "    decoder_batch.train()\n",
    "    return \"\".join(decoded_words), decoder_attentions[:di + 1]\n",
    "\n",
    "# –≠—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –Ω–æ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –Ω–æ–≤–æ–π evaluate\n",
    "def evaluate_randomly(n=5):\n",
    "    print(f\"\\nüéì –≠–ö–ó–ê–ú–ï–ù (Masked Model):\")\n",
    "    correct = 0\n",
    "    for i in range(n):\n",
    "        pair = random.choice(test_data)\n",
    "        src = pair[0]\n",
    "        tgt_real = pair[1]\n",
    "        \n",
    "        output_words, _ = evaluate(src)\n",
    "        \n",
    "        tgt_clean = tgt_real.replace('<','').replace('>','')\n",
    "        out_clean = output_words.replace('<EOS>','')\n",
    "        \n",
    "        is_match = \"‚úÖ\" if tgt_clean == out_clean else f\"‚ùå ({tgt_clean})\"\n",
    "        if tgt_clean == out_clean: correct += 1\n",
    "        print(f\"{src.ljust(15)} -> {out_clean.ljust(12)} {is_match}\")\n",
    "    print(f\"–¢–æ—á–Ω–æ—Å—Ç—å: {correct}/{n}\")\n",
    "\n",
    "def showAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    clean_output = output_words.replace('<EOS>', '')\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticklabels([''] + list(input_sentence), rotation=90)\n",
    "    ax.set_yticklabels([''] + list(clean_output))\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# === –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢ ===\n",
    "evaluate_randomly(10)\n",
    "print(\"\\n--- –ü—Ä–æ–≤–µ—Ä–∫–∞ –í–Ω–∏–º–∞–Ω–∏—è ---\")\n",
    "showAttention(\"1 Jan 2024\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
