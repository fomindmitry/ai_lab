{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd035308-996c-44cc-bda3-37681585fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version.cuda: None\n",
      "torch.cuda.is_available(): False\n",
      "torch.xpu.is_available(): True\n",
      "‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: xpu\n",
      "   –ö–∞—Ä—Ç–∞: Intel(R) Graphics [0x7d45]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import calendar\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU (–∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ —Ä–∞–Ω—å—à–µ)\n",
    "# –ï—Å–ª–∏ CUDA –µ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë. –ï—Å–ª–∏ –Ω–µ—Ç - –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä.\n",
    "\n",
    "print(f\"torch.version.cuda: {torch.version.cuda}\")       # –î–æ–ª–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å '12.4' –∏–ª–∏ '12.1' (–Ω–µ None!)\n",
    "print(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\") # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å True\n",
    "print(f\"torch.xpu.is_available(): {torch.xpu.is_available()}\") \n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ç—Ä–∏–±—É—Ç xpu, —á—Ç–æ–±—ã –∫–æ–¥ –Ω–µ –ø–∞–¥–∞–ª –Ω–∞ –º–∞—à–∏–Ω–∞—Ö —Å–æ —Å—Ç–∞—Ä—ã–º PyTorch\n",
    "    elif hasattr(torch, 'xpu') and torch.xpu.is_available():\n",
    "        return torch.device(\"xpu\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "print(f\"‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: {device}\")\n",
    "# –ï—Å–ª–∏ XPU –∞–∫—Ç–∏–≤–µ–Ω, –ø–æ–ª–µ–∑–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ –∏–º—è –∫–∞—Ä—Ç—ã:\n",
    "if device.type == 'xpu':\n",
    "    print(f\"   –ö–∞—Ä—Ç–∞: {torch.xpu.get_device_name(0)}\")\n",
    "elif device.type == 'cuda':\n",
    "    print(f\"   –ö–∞—Ä—Ç–∞: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bfafb8-4f0a-4b8b-b468-dcfef7803c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version: 3.12.3 (main, Nov  6 2025, 13:44:16) [GCC 13.3.0]\n",
      "platform.python_version(): 3.12.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"sys.version: {sys.version}\")\n",
    "\n",
    "print(f\"platform.python_version(): {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71669525-1843-4572-814a-10da7b8e4cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('22 Sep 2005', '<2005-09-22>')\n"
     ]
    }
   ],
   "source": [
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug',\n",
    "          'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "\n",
    "def generate_date():\n",
    "    year = random.randint(1950, 2050)\n",
    "\n",
    "    monthIdx = random.randint(1, 12)\n",
    "\n",
    "    match monthIdx:\n",
    "        case 1 | 3 | 5 | 7 | 8 | 10 | 12:\n",
    "            day = random.randint(1, 31)\n",
    "        case 4 | 6 | 9 | 11:\n",
    "            day = random.randint(1, 30)\n",
    "        case 2:\n",
    "            if calendar.isleap(year):\n",
    "                day = random.randint(1, 29)\n",
    "            else:\n",
    "                day = random.randint(1, 28)\n",
    "\n",
    "    # 1. –ü–æ—Ä—è–¥–æ–∫: –ì–æ–¥ - –ú–µ—Å—è—Ü - –î–µ–Ω—å\n",
    "    # 2. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ :02d –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–ª—å –≤ –Ω–∞—á–∞–ª–µ, –µ—Å–ª–∏ —á–∏—Å–ª–æ < 10 (–Ω–∞–ø—Ä–∏–º–µ—Ä, 05)\n",
    "    tgt_str = f'{year}-{monthIdx:02d}-{day:02d}'\n",
    "\n",
    "    # –í—Ö–æ–¥ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å: \"25 Jan 2023\"\n",
    "    src_str = f'{day} {months[monthIdx-1]} {year}'\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å—Ç–∞—Ä—Ç–∞ –∏ —Å—Ç–æ–ø–∞\n",
    "    tgt_str = f'<{tgt_str}>'\n",
    "\n",
    "    return src_str, tgt_str\n",
    "\n",
    "\n",
    "print(generate_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaff16cf-22d2-4ca1-9172-e19bd77d115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char2idx: {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, ' ': 3, '-': 4, '0': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '8': 13, '9': 14, '<': 15, '>': 16, 'A': 17, 'B': 18, 'C': 19, 'D': 20, 'E': 21, 'F': 22, 'G': 23, 'H': 24, 'I': 25, 'J': 26, 'K': 27, 'L': 28, 'M': 29, 'N': 30, 'O': 31, 'P': 32, 'Q': 33, 'R': 34, 'S': 35, 'T': 36, 'U': 37, 'V': 38, 'W': 39, 'X': 40, 'Y': 41, 'Z': 42, 'a': 43, 'b': 44, 'c': 45, 'd': 46, 'e': 47, 'f': 48, 'g': 49, 'h': 50, 'i': 51, 'j': 52, 'k': 53, 'l': 54, 'm': 55, 'n': 56, 'o': 57, 'p': 58, 'q': 59, 'r': 60, 's': 61, 't': 62, 'u': 63, 'v': 64, 'w': 65, 'x': 66, 'y': 67, 'z': 68}\n",
      "idx2char: {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: ' ', 4: '-', 5: '0', 6: '1', 7: '2', 8: '3', 9: '4', 10: '5', 11: '6', 12: '7', 13: '8', 14: '9', 15: '<', 16: '>', 17: 'A', 18: 'B', 19: 'C', 20: 'D', 21: 'E', 22: 'F', 23: 'G', 24: 'H', 25: 'I', 26: 'J', 27: 'K', 28: 'L', 29: 'M', 30: 'N', 31: 'O', 32: 'P', 33: 'Q', 34: 'R', 35: 'S', 36: 'T', 37: 'U', 38: 'V', 39: 'W', 40: 'X', 41: 'Y', 42: 'Z', 43: 'a', 44: 'b', 45: 'c', 46: 'd', 47: 'e', 48: 'f', 49: 'g', 50: 'h', 51: 'i', 52: 'j', 53: 'k', 54: 'l', 55: 'm', 56: 'n', 57: 'o', 58: 'p', 59: 'q', 60: 'r', 61: 's', 62: 't', 63: 'u', 64: 'v', 65: 'w', 66: 'x', 67: 'y', 68: 'z'}\n",
      "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤): 69\n",
      "–ò–Ω–¥–µ–∫—Å –±—É–∫–≤—ã 'A': 17\n",
      "–°–∏–º–≤–æ–ª –ø–æ–¥ –∏–Ω–¥–µ–∫—Å–æ–º 10: 5\n",
      "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Å —Ç–æ–∫–µ–Ω–∞–º–∏: 69\n",
      "–ò–Ω–¥–µ–∫—Å <PAD>: 0\n",
      "–ò–Ω–¥–µ–∫—Å <EOS>: 2\n"
     ]
    }
   ],
   "source": [
    "# 0. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã (–∑–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º –∏—Ö –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞)\n",
    "# –í–∞–∂–Ω–æ: 0-–π –∏–Ω–¥–µ–∫—Å —á–∞—Å—Ç–æ —Ä–µ–∑–µ—Ä–≤–∏—Ä—É—é—Ç –¥–ª—è PAD, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "# –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω—É–ª—è–º–∏.\n",
    "PAD_token = 0\n",
    "SOS_token = 1 \n",
    "EOS_token = 2\n",
    "\n",
    "# –°–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –±—É–¥—É—Ç –≤ –Ω–∞—à–µ–º —Å–ª–æ–≤–∞—Ä–µ\n",
    "special_chars = {\n",
    "    '<PAD>': PAD_token, \n",
    "    '<SOS>': SOS_token, \n",
    "    '<EOS>': EOS_token\n",
    "}\n",
    "\n",
    "\n",
    "# 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (Alphabet)\n",
    "# set() —É–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "chars = set(\"0123456789 abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ<>-\")\n",
    "\n",
    "# 2. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–∏–º–≤–æ–ª—ã –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è char2idx\n",
    "# –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å 3 (–ø–æ—Å–ª–µ 0, 1, 2)\n",
    "sorted_data_chars = sorted(chars)\n",
    "next_index = len(special_chars)\n",
    "\n",
    "# 3. –°–æ–∑–¥–∞–µ–º Map: –°–∏–º–≤–æ–ª -> –ò–Ω–¥–µ–∫—Å (Integer)\n",
    "# sorted() —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ–±—ã –ø–æ—Ä—è–¥–æ–∫ –≤—Å–µ–≥–¥–∞ –±—ã–ª –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º\n",
    "# enumerate() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä—ã (—Å—á–µ—Ç—á–∏–∫, —ç–ª–µ–º–µ–Ω—Ç)\n",
    "char2idx = special_chars.copy()\n",
    "char2idx.update({char: idx + next_index for idx, char in enumerate(sorted_data_chars)})\n",
    "print(f\"char2idx: {char2idx}\")\n",
    "\n",
    "# 3. –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π Map: –ò–Ω–¥–µ–∫—Å -> –°–∏–º–≤–æ–ª (–¥–ª—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞)\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "print(f\"idx2char: {idx2char}\")\n",
    "\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π —Ç–æ–∫–µ–Ω \"EOS\" (End Of Sentence) –∏–ª–∏ PAD, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "# –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–º–Ω–∏–º —Ä–∞–∑–º–µ—Ä –Ω–∞—à–µ–≥–æ \"–∞–ª—Ñ–∞–≤–∏—Ç–∞\"\n",
    "vocab_size = len(char2idx)\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤): {vocab_size}\")\n",
    "print(f\"–ò–Ω–¥–µ–∫—Å –±—É–∫–≤—ã 'A': {char2idx['A']}\")\n",
    "print(f\"–°–∏–º–≤–æ–ª –ø–æ–¥ –∏–Ω–¥–µ–∫—Å–æ–º 10: {idx2char[10]}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Å —Ç–æ–∫–µ–Ω–∞–º–∏: {vocab_size}\")\n",
    "print(f\"–ò–Ω–¥–µ–∫—Å <PAD>: {char2idx['<PAD>']}\")\n",
    "print(f\"–ò–Ω–¥–µ–∫—Å <EOS>: {char2idx['<EOS>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faafb3de-4775-4951-8f19-b5ef95865d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch Encoder –≥–æ—Ç–æ–≤.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderRNN_Batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN_Batch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # –ì—Ä—É–∑–∏–º –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ä–∞–∑—É\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input shape: (Seq_Len, Batch_Size)  <-- –ë—ã–ª–æ (1, 1)\n",
    "        \n",
    "        # 1. Embeddings\n",
    "        # –†–∞–Ω—å—à–µ –º—ã –¥–µ–ª–∞–ª–∏ .view(1, 1, -1). –¢–µ–ø–µ—Ä—å —ç—Ç–æ –Ω–µ –Ω—É–∂–Ω–æ.\n",
    "        # PyTorch —Å–∞–º –ø–æ–π–º–µ—Ç: (12 —Å—Ç—Ä–æ–∫, 32 –ø—Ä–∏–º–µ—Ä–∞) -> (12, 32, 128)\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # 2. GRU\n",
    "        # –ú—ã —Å–∫–∞—Ä–º–ª–∏–≤–∞–µ–º –ï–ú–£ –í–°–Æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ä–∞–∑—É!\n",
    "        # output: (Seq_Len, Batch_Size, Hidden_Size) - —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ\n",
    "        # hidden: (1, Batch_Size, Hidden_Size) - –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (—Å—É—Ç—å –≤—Å–µ–π —Ñ—Ä–∞–∑—ã)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        # –¢–µ–ø–µ—Ä—å –º—ã —Å–æ–∑–¥–∞–µ–º –ø–∞–º—è—Ç—å –Ω–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞, –∞ –¥–ª—è 32 —Å—Ä–∞–∑—É\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "print(\"‚úÖ Batch Encoder –≥–æ—Ç–æ–≤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5555b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch Decoder –≥–æ—Ç–æ–≤.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AttnDecoderRNN_Batch(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length=15):\n",
    "        super(AttnDecoderRNN_Batch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        \n",
    "        # –°–ª–æ–∏ –æ—Å—Ç–∞–ª–∏—Å—å —Ç–µ–º–∏ –∂–µ, —á—Ç–æ –∏ —Ä–∞–Ω—å—à–µ\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input: (1, Batch_Size) - —Ç–µ–∫—É—â–∏–µ –±—É–∫–≤—ã –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –±–∞—Ç—á–µ\n",
    "        # hidden: (1, Batch_Size, Hidden) - –ø–∞–º—è—Ç—å —Å –ø—Ä–æ—à–ª–æ–≥–æ —à–∞–≥–∞\n",
    "        # encoder_outputs: (Seq_Len, Batch, Hidden) - –í–ê–ñ–ù–û! –¢—É—Ç –≤—Ä–µ–º—è –ø–µ—Ä–≤–æ–µ.\n",
    "        \n",
    "        # 1. Embeddings\n",
    "        # (1, Batch) -> (1, Batch, Hidden)\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        # 2. –°—á–∏—Ç–∞–µ–º Attention Weights\n",
    "        # –ù–∞–º –Ω—É–∂–Ω–æ —Å–∫–ª–µ–∏—Ç—å (Batch, Hidden) –∏ (Batch, Hidden).\n",
    "        # –ë–µ—Ä–µ–º [0], —á—Ç–æ–±—ã –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è (1).\n",
    "        attn_input = torch.cat((embedded[0], hidden[0]), 1) \n",
    "        \n",
    "        # (Batch, Hidden*2) -> (Batch, Max_Len)\n",
    "        attn_weights = F.softmax(self.attn(attn_input), dim=1)\n",
    "\n",
    "        # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º Attention (–ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ)\n",
    "        # BMM —Ç—Ä–µ–±—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç (Batch, X, Y).\n",
    "        # encoder_outputs —É –Ω–∞—Å (Seq_Len, Batch, Hidden). –ù–∞–¥–æ –ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç—å!\n",
    "        # .transpose(0, 1) –¥–µ–ª–∞–µ—Ç (Batch, Seq_Len, Hidden)\n",
    "        encoder_outputs_transposed = encoder_outputs.transpose(0, 1)\n",
    "        \n",
    "        # (Batch, 1, Max_Len) x (Batch, Max_Len, Hidden) -> (Batch, 1, Hidden)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n",
    "                                 encoder_outputs_transposed)\n",
    "\n",
    "        # 4. Combine & GRU\n",
    "        # –°–∫–ª–µ–∏–≤–∞–µ–º (Batch, Hidden) –∏ (Batch, Hidden) -> (Batch, Hidden*2)\n",
    "        output = torch.cat((embedded[0], attn_applied[:, 0, :]), 1)\n",
    "        \n",
    "        output = self.attn_combine(output).unsqueeze(0) # -> (1, Batch, Hidden)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        # 5. Output\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "print(\"‚úÖ Batch Decoder –≥–æ—Ç–æ–≤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d83fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–∞—Ä–∫–µ—Ä –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏ (End Of Sentence).\n",
    "# –í –Ω–∞—à–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ –º—ã –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –¥–∞—Ç—É –≤ <...>.\n",
    "# –ó–Ω–∞—á–∏—Ç, —Å–∏–º–≤–æ–ª–æ–º –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –±—É–¥–µ—Ç '>'.\n",
    "EOS_token = char2idx['>']\n",
    "\n",
    "def tensorFromSentence(sentence):\n",
    "    # 1. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "    indexes = [char2idx[char] for char in sentence]\n",
    "    \n",
    "    # 2. –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä –∫–æ–Ω—Ü–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞)\n",
    "    # –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ '>' —É–∂–µ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏ –æ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞, \n",
    "    # –Ω–æ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–Ω —Ç–∞–º –µ—Å—Ç—å.\n",
    "    # –ù–∞—à –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤—ã–¥–∞–µ—Ç '<...>', —Ç–∞–∫ —á—Ç–æ '>' —É–∂–µ –≤–Ω—É—Ç—Ä–∏ indexes.\n",
    "    \n",
    "    # 3. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –¢–µ–Ω–∑–æ—Ä\n",
    "    # dtype=torch.long –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è –∏–Ω–¥–µ–∫—Å–æ–≤ (Embedding —Å–ª–æ–π —Ç—Ä–µ–±—É–µ—Ç long)\n",
    "    # .view(-1, 1) –¥–µ–ª–∞–µ—Ç –∏–∑ –≤–µ–∫—Ç–æ—Ä–∞ —Å—Ç–æ–ª–±–∏–∫ (Sequence Length x Batch Size)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917216c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(input_batch, target_batch, encoder, decoder, \n",
    "                encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    # –°–±—Ä–æ—Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # –†–∞–∑–º–µ—Ä—ã\n",
    "    batch_size = input_batch.size(1)\n",
    "    target_len = target_batch.size(0) # –î–ª–∏–Ω–∞ —Å–∞–º–æ–π –¥–ª–∏–Ω–Ω–æ–π —Ñ—Ä–∞–∑—ã –≤ –æ—Ç–≤–µ—Ç–µ\n",
    "\n",
    "    # 1. –ó–∞–ø—É—Å–∫ –≠–Ω–∫–æ–¥–µ—Ä–∞\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å —Å—Ä–∞–∑—É –¥–ª—è –≤—Å–µ–π –ø–∞—á–∫–∏\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    \n",
    "    # –ü—Ä–æ–≥–æ–Ω—è–µ–º –≤–µ—Å—å –±–∞—Ç—á —Ä–∞–∑–æ–º\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "    \n",
    "    # FIX –î–õ–Ø ATTENTION:\n",
    "    # –≠–Ω–∫–æ–¥–µ—Ä –º–æ–≥ –≤–µ—Ä–Ω—É—Ç—å –¥–ª–∏–Ω—É 12, –∞ –î–µ–∫–æ–¥–µ—Ä –∂–¥–µ—Ç 15 (max_length).\n",
    "    # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä-–∑–∞–≥–ª—É—à–∫—É –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.\n",
    "    # (Max_Len, Batch, Hidden)\n",
    "    proj_encoder_outputs = torch.zeros(decoder.max_length, batch_size, encoder.hidden_size, device=device)\n",
    "    \n",
    "    # –ö–æ–ø–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä—å –∑–∞–≥–ª—É—à–∫–∏\n",
    "    actual_len = encoder_outputs.size(0)\n",
    "    proj_encoder_outputs[:actual_len, :, :] = encoder_outputs\n",
    "\n",
    "    # 2. –ó–∞–ø—É—Å–∫ –î–µ–∫–æ–¥–µ—Ä–∞\n",
    "    # –ù–∞—á–∞–ª—å–Ω—ã–π –≤—Ö–æ–¥: —Å—Ç—Ä–æ–∫–∞ –∏–∑ <SOS> —Ç–æ–∫–µ–Ω–æ–≤ (—Ä–∞–∑–º–µ—Ä = batch_size)\n",
    "    decoder_input = torch.tensor([[SOS_token] * batch_size], device=device) # (1, Batch)\n",
    "    decoder_hidden = encoder_hidden # –ù–∞—Å–ª–µ–¥—É–µ–º –ø–∞–º—è—Ç—å\n",
    "    \n",
    "    loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # Teacher Forcing: –ø—Ä–∏–º–µ–Ω—è–µ–º —Å—Ä–∞–∑—É –∫–æ –≤—Å–µ–º—É –±–∞—Ç—á—É (–¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏)\n",
    "    # –í –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏—è—Ö –∏–Ω–æ–≥–¥–∞ –º–∏–∫—Å—É—é—Ç, –Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —ç—Ç–æ –æ–∫.\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # –ö–æ—Ä–º–∏–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏\n",
    "        for di in range(target_len):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, proj_encoder_outputs)\n",
    "            \n",
    "            # target_batch[di] - —ç—Ç–æ —Å—Ä–µ–∑ (Batch_Size) –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –±—É–∫–≤ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —à–∞–≥–∞\n",
    "            loss += criterion(decoder_output, target_batch[di])\n",
    "            \n",
    "            # –°–ª–µ–¥—É—é—â–∏–π –≤—Ö–æ–¥ - –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å—Ä–µ–∑ (unsqueeze –¥–æ–±–∞–≤–ª—è–µ—Ç –∏–∑–º–µ—Ä–µ–Ω–∏–µ 1, —á—Ç–æ–±—ã —Å—Ç–∞–ª–æ 1xBatch)\n",
    "            decoder_input = target_batch[di].unsqueeze(0) \n",
    "            \n",
    "    else:\n",
    "        # –ö–æ—Ä–º–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n",
    "        for di in range(target_len):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, proj_encoder_outputs)\n",
    "            \n",
    "            loss += criterion(decoder_output, target_batch[di])\n",
    "            \n",
    "            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ –±—É–∫–≤—ã\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            # topi –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä (Batch, 1). –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –≤ (1, Batch) –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞\n",
    "            decoder_input = topi.transpose(0, 1).detach()\n",
    "\n",
    "    # 3. Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # –í–∞–∂–Ω–æ: –∫–ª–∏–ø–ø–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (—á—Ç–æ–±—ã –Ω–µ –≤–∑—Ä—ã–≤–∞–ª–∏—Å—å –æ—Ç –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π)\n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), 5.0)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), 5.0)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6887b28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 10000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä...\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ!\n",
      "üìö –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ (Train): 8000 —à—Ç.\n",
      "üéì –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ (Test):  2000 —à—Ç. (–ú–æ–¥–µ–ª—å –∏—Ö –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —É–≤–∏–¥–∏—Ç)\n",
      "–ü—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ—Å—Ç–∞: ('19 Jul 2045', '<2045-07-19>')\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "TOTAL_SAMPLES = 10000  # –°–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ö–æ—Ç–∏–º\n",
    "TRAIN_RATIO = 0.8      # 80% –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "# 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—É–ª –¥–∞–Ω–Ω—ã—Ö\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º set, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –¥–∞—Ç—ã)\n",
    "dataset_set = set()\n",
    "\n",
    "print(f\"üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {TOTAL_SAMPLES} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä...\")\n",
    "\n",
    "while len(dataset_set) < TOTAL_SAMPLES:\n",
    "    # –í—ã–∑—ã–≤–∞–µ–º —Ç–≤–æ—é —Ñ—É–Ω–∫—Ü–∏—é (–æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –≤—ã—à–µ)\n",
    "    pair = generate_date() \n",
    "    dataset_set.add(pair)\n",
    "\n",
    "# –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º\n",
    "full_dataset = list(dataset_set)\n",
    "random.shuffle(full_dataset)\n",
    "\n",
    "# 2. –†–∞–∑—Ä–µ–∑–∞–µ–º –Ω–∞ Train –∏ Test\n",
    "split_index = int(TOTAL_SAMPLES * TRAIN_RATIO)\n",
    "\n",
    "train_data = full_dataset[:split_index]\n",
    "test_data = full_dataset[split_index:]\n",
    "\n",
    "print(f\"‚úÖ –ì–æ—Ç–æ–≤–æ!\")\n",
    "print(f\"üìö –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ (Train): {len(train_data)} —à—Ç.\")\n",
    "print(f\"üéì –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ (Test):  {len(test_data)} —à—Ç. (–ú–æ–¥–µ–ª—å –∏—Ö –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —É–≤–∏–¥–∏—Ç)\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ—Å—Ç–∞: {test_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3089181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§–æ—Ä–º–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ (Seq_Len, Batch): torch.Size([12, 3])\n",
      "–§–æ—Ä–º–∞ —Ü–µ–ª–µ–≤–æ–≥–æ –±–∞—Ç—á–∞ (Seq_Len, Batch): torch.Size([11, 3])\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ (–≤–∏–¥–∏–º –Ω—É–ª–∏-–ø–∞–¥–¥–∏–Ω–≥–∏ –≤ –∫–æ–Ω—Ü–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑?):\n",
      "tensor([[ 8,  5,  3, 31, 45, 62,  3,  7,  5,  7, 14, 16],\n",
      "        [ 7,  6,  3, 20, 47, 45,  3,  7,  5,  7, 11, 16],\n",
      "        [ 7, 14,  3, 35, 47, 58,  3,  7,  5,  9, 13, 16]], device='xpu:0')\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –°—Ç—Ä–æ–∫–∞ -> –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "def indexesFromSentence(sentence):\n",
    "    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –º–∞—Ä–∫–µ—Ä—ã < –∏ >, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å, —á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞—Ç—å—Å—è\n",
    "    clean_sentence = sentence.replace('<', '').replace('>', '')\n",
    "    \n",
    "    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ —Ü–∏—Ñ—Ä—ã\n",
    "    # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –≤—Å—Ç—Ä–µ—Ç–∏–º –Ω–µ–∑–Ω–∞–∫–æ–º—ã–π —Å–∏–º–≤–æ–ª, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ (–∏–ª–∏ –ø–∞–¥–∞–µ–º, –Ω–æ –ø–æ–∫–∞ –ø—Ä–æ–ø—É—Å—Ç–∏–º)\n",
    "    return [char2idx[char] for char in clean_sentence if char in char2idx] + [EOS_token]\n",
    "\n",
    "def get_batch(batch_size=32):\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    # 1. –ù–∞–±–∏—Ä–∞–µ–º batch_size —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "    for _ in range(batch_size):\n",
    "        pair = random.choice(train_data) # –ë–µ—Ä–µ–º –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "        input_str, target_str = pair\n",
    "        \n",
    "        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ —Å–ø–∏—Å–∫–∏ —á–∏—Å–µ–ª (–ù–ï —Ç–µ–Ω–∑–æ—Ä—ã –ø–æ–∫–∞)\n",
    "        input_idxs = indexesFromSentence(input_str)\n",
    "        target_idxs = indexesFromSentence(target_str)\n",
    "        \n",
    "        # –í–∞–∂–Ω–æ: –¥–ª—è PyTorch –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–∫–∏ –≤ —Ç–µ–Ω–∑–æ—Ä—ã\n",
    "        input_list.append(torch.tensor(input_idxs, dtype=torch.long, device=device))\n",
    "        target_list.append(torch.tensor(target_idxs, dtype=torch.long, device=device))\n",
    "        \n",
    "    # 2. –ú–∞–≥–∏—è PAD_SEQUENCE\n",
    "    # –û–Ω–∞ –±–µ—Ä–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä–æ–≤ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –¥–µ–ª–∞–µ—Ç –∏–∑ –Ω–∏—Ö –æ–¥–∏–Ω –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä.\n",
    "    # padding_value=PAD_token (–Ω–∞—à 0) ‚Äî —á–µ–º –∑–∞–ø–æ–ª–Ω—è—Ç—å –ø—É—Å—Ç–æ—Ç—É.\n",
    "    # –í–ê–ñ–ù–û: –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–Ω–∞ –¥–µ–ª–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (Max_Length, Batch_Size).\n",
    "    # –≠—Ç–æ —Ç–æ, —á—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è RNN (–≤—Ä–µ–º—è –∏–¥–µ—Ç –≤–Ω–∏–∑, –±–∞—Ç—á–∏ –∏–¥—É—Ç –≤–ø—Ä–∞–≤–æ).\n",
    "    input_batch = pad_sequence(input_list, padding_value=PAD_token)\n",
    "    target_batch = pad_sequence(target_list, padding_value=PAD_token)\n",
    "    \n",
    "    return input_batch, target_batch\n",
    "\n",
    "# === –¢–ï–°–¢ ===\n",
    "# –î–∞–≤–∞–π –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –æ–¥–∏–Ω –±–∞—Ç—á\n",
    "inp, tgt = get_batch(batch_size=3)\n",
    "print(f\"–§–æ—Ä–º–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ (Seq_Len, Batch): {inp.shape}\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ü–µ–ª–µ–≤–æ–≥–æ –±–∞—Ç—á–∞ (Seq_Len, Batch): {tgt.shape}\")\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ (–≤–∏–¥–∏–º –Ω—É–ª–∏-–ø–∞–¥–¥–∏–Ω–≥–∏ –≤ –∫–æ–Ω—Ü–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑?):\")\n",
    "print(inp.transpose(0, 1)) # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ (Batch, Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b115248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Ö–æ–¥–Ω–æ–π –±–∞—Ç—á: torch.Size([12, 5]) (Seq_Len, Batch)\n",
      "–í—ã—Ö–æ–¥ –≤—Å–µ—Ö —à–∞–≥–æ–≤ (Output): torch.Size([12, 5, 128]) (Seq_Len, Batch, Hidden)\n",
      "–§–∏–Ω–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å (Hidden): torch.Size([1, 5, 128]) (1, Batch, Hidden)\n",
      "‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω: –≠–Ω–∫–æ–¥–µ—Ä —Å—ä–µ–ª –º–∞—Ç—Ä–∏—Ü—É –∏ –Ω–µ –ø–æ–ø–µ—Ä—Ö–Ω—É–ª—Å—è.\n"
     ]
    }
   ],
   "source": [
    "# 1. –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á –∏–∑ 5 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "BATCH_SIZE = 5\n",
    "inp_batch, tgt_batch = get_batch(BATCH_SIZE)\n",
    "# inp_batch –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä (Seq_Len, 5)\n",
    "\n",
    "# 2. –°–æ–∑–¥–∞–µ–º —ç–Ω–∫–æ–¥–µ—Ä\n",
    "test_enc_batch = EncoderRNN_Batch(vocab_size, hidden_size=128).to(device)\n",
    "\n",
    "# 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å (—Ç–µ–ø–µ—Ä—å –ø–µ—Ä–µ–¥–∞–µ–º batch_size!)\n",
    "hidden = test_enc_batch.initHidden(BATCH_SIZE)\n",
    "\n",
    "# 4. –ü—Ä–æ–≥–æ–Ω—è–µ–º\n",
    "enc_output, enc_hidden = test_enc_batch(inp_batch, hidden)\n",
    "\n",
    "print(f\"–í—Ö–æ–¥–Ω–æ–π –±–∞—Ç—á: {inp_batch.shape} (Seq_Len, Batch)\")\n",
    "print(f\"–í—ã—Ö–æ–¥ –≤—Å–µ—Ö —à–∞–≥–æ–≤ (Output): {enc_output.shape} (Seq_Len, Batch, Hidden)\")\n",
    "print(f\"–§–∏–Ω–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å (Hidden): {enc_hidden.shape} (1, Batch, Hidden)\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞: Batch dimension (–≤—Ç–æ—Ä–∞—è —Ü–∏—Ñ—Ä–∞) –≤–µ–∑–¥–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 5\n",
    "assert enc_output.shape[1] == BATCH_SIZE\n",
    "assert enc_hidden.shape[1] == BATCH_SIZE\n",
    "print(\"‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω: –≠–Ω–∫–æ–¥–µ—Ä —Å—ä–µ–ª –º–∞—Ç—Ä–∏—Ü—É –∏ –Ω–µ –ø–æ–ø–µ—Ä—Ö–Ω—É–ª—Å—è.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcbffaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Input shape: torch.Size([1, 5])\n",
      "Decoder Output shape: torch.Size([5, 69]) (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å Batch x Vocab)\n",
      "Attention shape: torch.Size([5, 15]) (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å Batch x Max_Len)\n",
      "‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω: –î–µ–∫–æ–¥–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø–∞–∫–µ—Ç–∞–º–∏!\n"
     ]
    }
   ],
   "source": [
    "# 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "BATCH_SIZE = 5\n",
    "MAX_LEN = 15 # –î–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å max_length –≤ –î–µ–∫–æ–¥–µ—Ä–µ (—É –Ω–∞—Å —Ç–∞–º —Ö–∞—Ä–¥–∫–æ–¥ 15 –ø–æ–∫–∞)\n",
    "\n",
    "# 2. –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏\n",
    "test_enc = EncoderRNN_Batch(vocab_size, 128).to(device)\n",
    "test_dec = AttnDecoderRNN_Batch(128, vocab_size, max_length=MAX_LEN).to(device)\n",
    "\n",
    "# 3. –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ\n",
    "# –í—Ö–æ–¥: (12, 5) - –¥–ª–∏–Ω–∞ 12, –±–∞—Ç—á 5\n",
    "inp, tgt = get_batch(BATCH_SIZE) \n",
    "\n",
    "# 4. –ü—Ä–æ–≥–æ–Ω –≠–Ω–∫–æ–¥–µ—Ä–∞\n",
    "enc_out, enc_hidden = test_enc(inp, test_enc.initHidden(BATCH_SIZE))\n",
    "# enc_out —Å–µ–π—á–∞—Å –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä (–†–µ–∞–ª—å–Ω–∞—è_–î–ª–∏–Ω–∞, 5, 128).\n",
    "# –ù–û! –î–µ–∫–æ–¥–µ—Ä –∂–¥–µ—Ç, —á—Ç–æ encoder_outputs –±—É–¥–µ—Ç —Ä–æ–≤–Ω–æ MAX_LENGTH (15).\n",
    "# –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ —ç—Ç–æ —Ä–µ—à–∞–µ—Ç—Å—è –º–∞—Å–∫–∞–º–∏, –Ω–æ –º—ã —Å–¥–µ–ª–∞–µ–º Pad –ø—Ä—è–º–æ –∑–¥–µ—Å—å –¥–ª—è —Ç–µ—Å—Ç–∞.\n",
    "# –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (15, 5, 128) –∏ –∫–æ–ø–∏—Ä—É–µ–º —Ç—É–¥–∞ –¥–∞–Ω–Ω—ã–µ.\n",
    "pad_enc_out = torch.zeros(MAX_LEN, BATCH_SIZE, 128, device=device)\n",
    "# –ö–æ–ø–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É\n",
    "length = enc_out.size(0)\n",
    "pad_enc_out[:length, :, :] = enc_out\n",
    "\n",
    "# 5. –ü—Ä–æ–≥–æ–Ω –î–µ–∫–æ–¥–µ—Ä–∞ (–û–¥–∏–Ω —à–∞–≥)\n",
    "dec_input = torch.tensor([[SOS_token] * BATCH_SIZE], device=device) # (1, 5)\n",
    "dec_hidden = enc_hidden # –ü–µ—Ä–µ–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "\n",
    "dec_out, dec_hidden_new, attn = test_dec(dec_input, dec_hidden, pad_enc_out)\n",
    "\n",
    "print(f\"Decoder Input shape: {dec_input.shape}\")\n",
    "print(f\"Decoder Output shape: {dec_out.shape} (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å Batch x Vocab)\")\n",
    "print(f\"Attention shape: {attn.shape} (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å Batch x Max_Len)\")\n",
    "\n",
    "assert dec_out.shape == (BATCH_SIZE, vocab_size)\n",
    "assert attn.shape == (BATCH_SIZE, MAX_LEN)\n",
    "print(\"‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω: –î–µ–∫–æ–¥–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø–∞–∫–µ—Ç–∞–º–∏!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c7bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ó–∞–ø—É—Å–∫ BATCH –æ–±—É—á–µ–Ω–∏—è (Batch Size=2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 14:57:16.438000 30025 torch/_inductor/utils.py:1613] [1/0_1] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# === –ù–ê–°–¢–†–û–ô–ö–ò ===\n",
    "BATCH_SIZE = 2048  # –¢–µ–ø–µ—Ä—å –º—ã —É—á–∏–º –ø–æ 64 –ø—Ä–∏–º–µ—Ä–∞ –∑–∞ —Ä–∞–∑!\n",
    "n_iters = 2000   # –ò—Ç–µ—Ä–∞—Ü–∏–π –º–µ–Ω—å—à–µ, —Ç.–∫. –∑–∞ –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é –º—ã –≤–∏–¥–∏–º 64 –ø—Ä–∏–º–µ—Ä–∞\n",
    "learning_rate = 0.005 # –ß—É—Ç—å —É–º–µ–Ω—å—à–∏–º, —Ç–∞–∫ –∫–∞–∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—É–º–º–∏—Ä—É—é—Ç—Å—è\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "# 1. –ú–æ–¥–µ–ª–∏\n",
    "encoder_batch = EncoderRNN_Batch(vocab_size, 128).to(device)\n",
    "decoder_batch = AttnDecoderRNN_Batch(128, vocab_size, max_length=15).to(device)\n",
    "\n",
    "encoder_batch.compile()\n",
    "decoder_batch.compile()\n",
    "\n",
    "# 2. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã\n",
    "enc_optim = optim.Adam(encoder_batch.parameters(), lr=learning_rate) # –ü–æ–ø—Ä–æ–±—É–µ–º Adam, –æ–Ω –±—ã—Å—Ç—Ä–µ–µ —Å—Ö–æ–¥–∏—Ç—Å—è\n",
    "dec_optim = optim.Adam(decoder_batch.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å –ò–ì–ù–û–†–û–ú –ü–ê–î–î–ò–ù–ì–û–í\n",
    "# –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ! –ú—ã –Ω–µ —à—Ç—Ä–∞—Ñ—É–µ–º –º–æ–¥–µ–ª—å –∑–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—É—Å—Ç–æ—Ç—ã –Ω–∞ –º–µ—Å—Ç–µ –ø—É—Å—Ç–æ—Ç—ã.\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_token)\n",
    "\n",
    "plot_losses = []\n",
    "print_every = 1\n",
    "current_loss = 0\n",
    "start = time.time()\n",
    "\n",
    "print(f\"üöÄ –ó–∞–ø—É—Å–∫ BATCH –æ–±—É—á–µ–Ω–∏—è (Batch Size={BATCH_SIZE})...\")\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—á–∫—É –¥–∞–Ω–Ω—ã—Ö\n",
    "    input_batch, target_batch = get_batch(BATCH_SIZE)\n",
    "    \n",
    "    # –®–∞–≥ –æ–±—É—á–µ–Ω–∏—è\n",
    "    loss = train_batch(input_batch, target_batch, encoder_batch, decoder_batch,\n",
    "                       enc_optim, dec_optim, criterion)\n",
    "    \n",
    "    current_loss += loss\n",
    "    plot_losses.append(loss)\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print_loss_avg = current_loss / print_every\n",
    "        current_loss = 0\n",
    "        # –°—á–∏—Ç–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å (–ø—Ä–∏–º–µ—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É)\n",
    "        # iter * batch_size = —Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–æ—à–ª–∏\n",
    "        speed = (iter * BATCH_SIZE) / (time.time() - start)\n",
    "        print(f'{iter} | Loss: {print_loss_avg:.4f} | Speed: {speed:.1f} samples/sec')\n",
    "\n",
    "print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ab308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (—Å–≤–µ—Ä—Ç–∫–∞)\n",
    "def moving_average(data, window_size=50):\n",
    "    # window_size=50 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–∞–∂–¥–∞—è —Ç–æ—á–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞ - —ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –∑–∞ 50 —à–∞–≥–æ–≤\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –∫ —Ç–≤–æ–µ–º—É —Å–ø–∏—Å–∫—É –æ—à–∏–±–æ–∫\n",
    "smooth_losses = moving_average(all_losses)\n",
    "\n",
    "# –†–∏—Å—É–µ–º\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(smooth_losses, label='Smoothed Loss')\n",
    "plt.title(\"–ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è (–°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ)\")\n",
    "plt.xlabel(\"–ò—Ç–µ—Ä–∞—Ü–∏–∏\")\n",
    "plt.ylabel(\"–û—à–∏–±–∫–∞\")\n",
    "plt.grid(True, alpha=0.3) # –î–æ–±–∞–≤–∏–º —Å–µ—Ç–∫—É –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    with torch.no_grad(): # –û—Ç–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞\n",
    "        input_tensor = tensorFromSentence(input_sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        # –ú–∞—Å—Å–∏–≤ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (–¥–ª—è –∫–∞—Ä—Ç–∏–Ω–∫–∏)\n",
    "        decoder_attentions = torch.zeros(decoder.max_length, decoder.max_length)\n",
    "\n",
    "        encoder_outputs = torch.zeros(decoder.max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        # 1. –ß–∏—Ç–∞–µ–º (Encoder)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        # 2. –ü–∏—à–µ–º (Decoder)\n",
    "        decoder_input = torch.tensor([[char2idx['<']]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(decoder.max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            \n",
    "            if topi.item() == char2idx['>']:\n",
    "                break # –ö–æ–Ω–µ—Ü —Ñ—Ä–∞–∑—ã\n",
    "            else:\n",
    "                decoded_words.append(idx2char[topi.item()])\n",
    "\n",
    "            decoder_input = topi.detach()\n",
    "\n",
    "        return \"\".join(decoded_words), decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly(n=5):\n",
    "    print(f\"\\nüéì –≠–ö–ó–ê–ú–ï–ù (–î–∞–Ω–Ω—ã–µ –∏–∑ Test Set):\")\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        # –ë–µ—Ä–µ–º –∏–∑ TEST data\n",
    "        pair = random.choice(test_data)\n",
    "        src = pair[0]\n",
    "        tgt_real = pair[1]\n",
    "        \n",
    "        # –ü—Ä–æ—Å–∏–º –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å\n",
    "        output_words, _ = evaluate(src) # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é evaluate\n",
    "        \n",
    "        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º (—É–±–∏—Ä–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã < > –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã)\n",
    "        tgt_clean = tgt_real.replace('<','').replace('>','')\n",
    "        out_clean = output_words.replace('<','').replace('>','')\n",
    "        \n",
    "        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ\n",
    "        is_match = \"‚úÖ\" if tgt_clean == out_clean else f\"‚ùå (–û–∂–∏–¥–∞–ª–æ—Å—å: {tgt_clean})\"\n",
    "        if tgt_clean == out_clean: correct += 1\n",
    "            \n",
    "        print(f\"–í—Ö–æ–¥: {src.ljust(15)} -> –í—ã—Ö–æ–¥: {out_clean} {is_match}\")\n",
    "        \n",
    "    print(f\"\\n–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã–±–æ—Ä–∫–µ: {correct}/{n}\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫–∑–∞–º–µ–Ω\n",
    "evaluate_randomly(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    \n",
    "    # –†–∏—Å—É–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # –ü–æ–¥–ø–∏—Å–∏ –æ—Å–µ–π\n",
    "    ax.set_xticklabels([''] + list(input_sentence), rotation=90)\n",
    "    ax.set_yticklabels([''] + list(output_words))\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    print(f\"–í—Ö–æ–¥:  {input_sentence}\")\n",
    "    print(f\"–í—ã—Ö–æ–¥: {output_words}\")\n",
    "\n",
    "# === –ó–ê–ü–£–°–ö –ü–†–û–í–ï–†–ö–ò ===\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥–∞—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª—å –ù–ï –≤–∏–¥–µ–ª–∞\n",
    "def showAttentions(n = 3):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(test_data)\n",
    "        src = pair[0]\n",
    "        showAttention(src)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
